{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnoz/Q3CnXLFzB/Mq/Vhh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwalr96/TenserFlow_Learning/blob/master/binaryImageClassifier_TestData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCbwiBRTcXY1"
      },
      "source": [
        "# Loading training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YBrZVwcTv5"
      },
      "source": [
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\r\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyTcgbZGck9W"
      },
      "source": [
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\r\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KhhddU3cm0w"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "\r\n",
        "local_zip = '/tmp/horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('/tmp/horse-or-human')\r\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\r\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6bwBl8ncpb9"
      },
      "source": [
        "# Directory with our training horse pictures\r\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\r\n",
        "\r\n",
        "# Directory with our training human pictures\r\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\r\n",
        "\r\n",
        "# Directory with our training horse pictures\r\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\r\n",
        "\r\n",
        "# Directory with our training human pictures\r\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avpqSLnrcrS5"
      },
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\r\n",
        "print(train_horse_names[:10])\r\n",
        "\r\n",
        "train_human_names = os.listdir(train_human_dir)\r\n",
        "print(train_human_names[:10])\r\n",
        "\r\n",
        "validation_horse_hames = os.listdir(validation_horse_dir)\r\n",
        "print(validation_horse_hames[:10])\r\n",
        "\r\n",
        "validation_human_names = os.listdir(validation_human_dir)\r\n",
        "print(validation_human_names[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPjjdTaqctWy"
      },
      "source": [
        "print('total training horse images:', len(os.listdir(train_horse_dir)))\r\n",
        "print('total training human images:', len(os.listdir(train_human_dir)))\r\n",
        "print('total validation horse images:', len(os.listdir(validation_horse_dir)))\r\n",
        "print('total validation human images:', len(os.listdir(validation_human_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XiNlnvccu-_"
      },
      "source": [
        "%matplotlib inline\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "\r\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\r\n",
        "nrows = 4\r\n",
        "ncols = 4\r\n",
        "\r\n",
        "# Index for iterating over images\r\n",
        "pic_index = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g7xNNIlcyHf"
      },
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\r\n",
        "fig = plt.gcf()\r\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\r\n",
        "\r\n",
        "pic_index += 8\r\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \r\n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\r\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \r\n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\r\n",
        "\r\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\r\n",
        "  # Set up subplot; subplot indices start at 1\r\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\r\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\r\n",
        "\r\n",
        "  img = mpimg.imread(img_path)\r\n",
        "  plt.imshow(img)\r\n",
        "\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvZDQnz0c18u"
      },
      "source": [
        "# Binary image classifier with test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQLZ04aOc1YS"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGZS12yec8q6"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\r\n",
        "    # This is the first convolution\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    # The second convolution\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The third convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fourth convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fifth convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # Flatten the results to feed into a DNN\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    # 512 neuron hidden layer\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmdxagDc-R-"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWYaW_hEdBje"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=RMSprop(lr=0.001),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZncqLojGdDj6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# All images will be rescaled by 1./255\r\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "\r\n",
        "# Flow training images in batches of 128 using train_datagen generator\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\r\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\r\n",
        "        batch_size=128,\r\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\r\n",
        "        class_mode='binary')\r\n",
        "\r\n",
        "# Flow training images in batches of 128 using train_datagen generator\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\r\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\r\n",
        "        batch_size=32,\r\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\r\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTzQAsIDdFJS"
      },
      "source": [
        "history = model.fit(\r\n",
        "      train_generator,\r\n",
        "      steps_per_epoch=8,  \r\n",
        "      epochs=15,\r\n",
        "      verbose=1,\r\n",
        "      validation_data = validation_generator,\r\n",
        "      validation_steps=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtCFPiKAdItq"
      },
      "source": [
        "import numpy as np\r\n",
        "from google.colab import files\r\n",
        "from keras.preprocessing import image\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "for fn in uploaded.keys():\r\n",
        " \r\n",
        "  # predicting images\r\n",
        "  path = '/content/' + fn\r\n",
        "  img = image.load_img(path, target_size=(300, 300))\r\n",
        "  x = image.img_to_array(img)\r\n",
        "  x = np.expand_dims(x, axis=0)\r\n",
        "\r\n",
        "  images = np.vstack([x])\r\n",
        "  classes = model.predict(images, batch_size=10)\r\n",
        "  print(classes[0])\r\n",
        "  if classes[0]>0.5:\r\n",
        "    print(fn + \" is a human\")\r\n",
        "  else:\r\n",
        "    print(fn + \" is a horse\")\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Lsp4ABdKyI"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\r\n",
        "\r\n",
        "# Let's define a new Model that will take an image as input, and will output\r\n",
        "# intermediate representations for all layers in the previous model after\r\n",
        "# the first.\r\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\r\n",
        "#visualization_model = Model(img_input, successive_outputs)\r\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\r\n",
        "# Let's prepare a random input image from the training set.\r\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\r\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\r\n",
        "img_path = random.choice(horse_img_files + human_img_files)\r\n",
        "\r\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\r\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\r\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\r\n",
        "\r\n",
        "# Rescale by 1/255\r\n",
        "x /= 255\r\n",
        "\r\n",
        "# Let's run our image through our network, thus obtaining all\r\n",
        "# intermediate representations for this image.\r\n",
        "successive_feature_maps = visualization_model.predict(x)\r\n",
        "\r\n",
        "# These are the names of the layers, so can have them as part of our plot\r\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\r\n",
        "\r\n",
        "# Now let's display our representations\r\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\r\n",
        "  if len(feature_map.shape) == 4:\r\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\r\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\r\n",
        "    # The feature map has shape (1, size, size, n_features)\r\n",
        "    size = feature_map.shape[1]\r\n",
        "    # We will tile our images in this matrix\r\n",
        "    display_grid = np.zeros((size, size * n_features))\r\n",
        "    for i in range(n_features):\r\n",
        "      # Postprocess the feature to make it visually palatable\r\n",
        "      x = feature_map[0, :, :, i]\r\n",
        "      x -= x.mean()\r\n",
        "      x /= x.std()\r\n",
        "      x *= 64\r\n",
        "      x += 128\r\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\r\n",
        "      # We'll tile each filter into this big horizontal grid\r\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\r\n",
        "    # Display the grid\r\n",
        "    scale = 20. / n_features\r\n",
        "    plt.figure(figsize=(scale * n_features, scale))\r\n",
        "    plt.title(layer_name)\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}